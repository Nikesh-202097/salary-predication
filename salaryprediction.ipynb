{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "import numpy as np \n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('usjobs_train.csv')\n",
    "test_data = pd.read_csv('usjobs_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Job</th>\n",
       "      <th>Jobs_Group</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Remote</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Frecuency_Salary</th>\n",
       "      <th>...</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Sector_Group</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Employee</th>\n",
       "      <th>Company_Score</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Director</th>\n",
       "      <th>Director_Score</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16148</th>\n",
       "      <td>job_2a7a50622c24a7ce</td>\n",
       "      <td>Data Scientist - Trust and Safety Product - USDS</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TikTok</td>\n",
       "      <td>Los Angeles, CA+38 locations</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>year</td>\n",
       "      <td>...</td>\n",
       "      <td>['PowerPoint', 'SQL', 'Python']</td>\n",
       "      <td>Audiovisual and Media Communications</td>\n",
       "      <td>Media Communications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XXXL</td>\n",
       "      <td>3.4</td>\n",
       "      <td>122.0</td>\n",
       "      <td>Shou Zi Chew</td>\n",
       "      <td>0.68</td>\n",
       "      <td>https://www.tiktok.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31565</th>\n",
       "      <td>job_4b423846f9afdebe</td>\n",
       "      <td>Business System Analyst</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buzzclan LLC</td>\n",
       "      <td>Hayward, CA 94545 (Mt Eden area)+1 location</td>\n",
       "      <td>Hayward</td>\n",
       "      <td>CA</td>\n",
       "      <td>hour</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>XXS</td>\n",
       "      <td>XS</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.buzzclan.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7126</th>\n",
       "      <td>job_9a45afdc8c75371f</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Booz Allen Hamilton</td>\n",
       "      <td>Melbourne, FL+9 ubicaciones</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>FL</td>\n",
       "      <td>year</td>\n",
       "      <td>...</td>\n",
       "      <td>['Agile', 'Bachelor']</td>\n",
       "      <td>Management and Consulting</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>XXL</td>\n",
       "      <td>XXXL</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2464.0</td>\n",
       "      <td>Horacio D. Rozanski</td>\n",
       "      <td>0.87</td>\n",
       "      <td>http://www.boozallen.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12226</th>\n",
       "      <td>job_6cc9a812b16ed2db</td>\n",
       "      <td>Assistant Controller</td>\n",
       "      <td>Controller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remote</td>\n",
       "      <td>Central Oregon Community College</td>\n",
       "      <td>Bend, OR 97701 (River West area)</td>\n",
       "      <td>Bend</td>\n",
       "      <td>OR</td>\n",
       "      <td>year</td>\n",
       "      <td>...</td>\n",
       "      <td>['English', 'Spanish', 'Master', 'Word', 'Acce...</td>\n",
       "      <td>Education</td>\n",
       "      <td>Education</td>\n",
       "      <td>S</td>\n",
       "      <td>L</td>\n",
       "      <td>4.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.cocc.edu/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31498</th>\n",
       "      <td>job_caec93625b745c26</td>\n",
       "      <td>Decision Scientist, Medicare Analytics</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CVS Health</td>\n",
       "      <td>New York, NY+2 locations</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>year</td>\n",
       "      <td>...</td>\n",
       "      <td>['PowerPoint', 'Python', 'MBA', 'Master', 'SQL...</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>Health</td>\n",
       "      <td>XXXL</td>\n",
       "      <td>XXXL</td>\n",
       "      <td>3.2</td>\n",
       "      <td>43596.0</td>\n",
       "      <td>Karen S. Lynch, President &amp; CEO</td>\n",
       "      <td>0.59</td>\n",
       "      <td>http://jobs.cvshealth.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                                               Job  \\\n",
       "16148  job_2a7a50622c24a7ce  Data Scientist - Trust and Safety Product - USDS   \n",
       "31565  job_4b423846f9afdebe                           Business System Analyst   \n",
       "7126   job_9a45afdc8c75371f                                  Business Analyst   \n",
       "12226  job_6cc9a812b16ed2db                              Assistant Controller   \n",
       "31498  job_caec93625b745c26            Decision Scientist, Medicare Analytics   \n",
       "\n",
       "             Jobs_Group Profile  Remote                           Company  \\\n",
       "16148    Data Scientist     NaN     NaN                            TikTok   \n",
       "31565  Business Analyst     NaN     NaN                      Buzzclan LLC   \n",
       "7126   Business Analyst     NaN  Remote               Booz Allen Hamilton   \n",
       "12226        Controller     NaN  Remote  Central Oregon Community College   \n",
       "31498    Data Scientist     NaN     NaN                        CVS Health   \n",
       "\n",
       "                                          Location         City State  \\\n",
       "16148                 Los Angeles, CA+38 locations  Los Angeles    CA   \n",
       "31565  Hayward, CA 94545 (Mt Eden area)+1 location      Hayward    CA   \n",
       "7126                   Melbourne, FL+9 ubicaciones    Melbourne    FL   \n",
       "12226             Bend, OR 97701 (River West area)         Bend    OR   \n",
       "31498                     New York, NY+2 locations     New York    NY   \n",
       "\n",
       "      Frecuency_Salary  ...  \\\n",
       "16148             year  ...   \n",
       "31565             hour  ...   \n",
       "7126              year  ...   \n",
       "12226             year  ...   \n",
       "31498             year  ...   \n",
       "\n",
       "                                                  Skills  \\\n",
       "16148                    ['PowerPoint', 'SQL', 'Python']   \n",
       "31565                                                 []   \n",
       "7126                               ['Agile', 'Bachelor']   \n",
       "12226  ['English', 'Spanish', 'Master', 'Word', 'Acce...   \n",
       "31498  ['PowerPoint', 'Python', 'MBA', 'Master', 'SQL...   \n",
       "\n",
       "                                     Sector            Sector_Group Revenue  \\\n",
       "16148  Audiovisual and Media Communications    Media Communications     NaN   \n",
       "31565                Information Technology  Information Technology     XXS   \n",
       "7126              Management and Consulting              Consulting     XXL   \n",
       "12226                             Education               Education       S   \n",
       "31498                            Healthcare                  Health    XXXL   \n",
       "\n",
       "      Employee Company_Score  Reviews                         Director  \\\n",
       "16148     XXXL           3.4    122.0                     Shou Zi Chew   \n",
       "31565       XS           2.3      4.0                              NaN   \n",
       "7126      XXXL           3.9   2464.0              Horacio D. Rozanski   \n",
       "12226        L           4.0     58.0                              NaN   \n",
       "31498     XXXL           3.2  43596.0  Karen S. Lynch, President & CEO   \n",
       "\n",
       "      Director_Score                         URL  \n",
       "16148           0.68     https://www.tiktok.com/  \n",
       "31565            NaN    http://www.buzzclan.com/  \n",
       "7126            0.87   http://www.boozallen.com/  \n",
       "12226            NaN        http://www.cocc.edu/  \n",
       "31498           0.59  http://jobs.cvshealth.com/  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)\n",
    "\n",
    "# randomly selecting a sample of 5 data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                      0\n",
       "Job                     0\n",
       "Jobs_Group              0\n",
       "Profile             14061\n",
       "Remote              12822\n",
       "Company                10\n",
       "Location                8\n",
       "City                 2625\n",
       "State                2119\n",
       "Frecuency_Salary        0\n",
       "Skills                  0\n",
       "Sector               4735\n",
       "Sector_Group         4735\n",
       "Revenue             12022\n",
       "Employee             8418\n",
       "Company_Score        5828\n",
       "Reviews              5828\n",
       "Director            13661\n",
       "Director_Score      14464\n",
       "URL                 10519\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.isna().sum()\n",
    "\n",
    "# identify and count the missing values (NaN or null values) in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = test_data['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(_data_):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor        # ensemble learning method for regression tasks\n",
    "    from sklearn.impute import SimpleImputer                  # imputing missing values in a dataset\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.metrics import mean_squared_error            # evaluate the performance of regression models\n",
    "    from sklearn.preprocessing import LabelEncoder            # convert categorical labels into numerical labels\n",
    "    from sklearn.preprocessing import MinMaxScaler            # scales features to a specified range (usually between 0 and 1)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Removes duplicate rows based on the 'ID' and 'Job' columns\n",
    "    \n",
    "    _data_.drop_duplicates(subset=['ID', 'Job'], inplace=True)  #inplace=True (changes should be made directly to the original DataFrame)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Drops unnecessary columns from the dataset\n",
    "    \n",
    "    _data_.drop(['ID','Job','URL','Director','State','City','Company','Sector','Location'],inplace = True,axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Fills missing values in 'Profile', 'Remote', 'Sector_Group', and 'Revenue' columns with specified default values\n",
    "    \n",
    "    # train_data.dropna(subset=['Location'],inplace = True)\n",
    "    _data_['Profile'].fillna('Not Specified', inplace=True)\n",
    "    _data_['Remote'].fillna('Not Specified', inplace=True)\n",
    "    \n",
    "    \n",
    "    # Fill null values in 'Sector_Group' with 'Not Specified'\n",
    "    _data_['Sector_Group'].fillna('Not Specified', inplace=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Categorical Data Processing\n",
    "\n",
    "    # Define a threshold for considering significant categories\n",
    "    threshold = 800\n",
    "\n",
    "    # creating a list of categories in the 'Sector_Group' column that occur less frequently than specified threshold\n",
    "    infrequent_categories = _data_['Sector_Group'].value_counts()[_data_['Sector_Group'].value_counts() < threshold].index\n",
    "\n",
    "    # Replace infrequent categories with 'Others'\n",
    "    _data_['Sector_Group'] = np.where(_data_['Sector_Group'].isin(infrequent_categories), 'Others', _data_['Sector_Group'])\n",
    "    \n",
    "    # Impute missing values in the 'Revenue' column with the most common category ('XXXL')\n",
    "    _data_['Revenue'].fillna('XXXL', inplace=True)\n",
    "\n",
    "    # Group categories into broader ranges\n",
    "    revenue_mapping = {\n",
    "        'XXXS': 'Low',\n",
    "        'XXS': 'Low',\n",
    "        'XS': 'Low',\n",
    "        'S': 'Medium',\n",
    "        'M': 'Medium',\n",
    "        'L': 'Medium',\n",
    "        'XL': 'High',\n",
    "        'XXL': 'High',\n",
    "        'XXXL': 'High'\n",
    "    }\n",
    "\n",
    "    _data_['Revenue_Grouped'] = _data_['Revenue'].map(revenue_mapping)\n",
    "    \n",
    "        \n",
    "    # Impute missing values in the 'Employee' column with the most common category ('XXXL')\n",
    "    _data_['Employee'].fillna('XXXL', inplace=True)\n",
    "\n",
    "    # Group categories into broader ranges\n",
    "    employee_mapping = {\n",
    "        'XXXS': 'Small',\n",
    "        'XXS': 'Small',\n",
    "        'XS': 'Small',\n",
    "        'S': 'Medium',\n",
    "        'M': 'Medium',\n",
    "        'L': 'Large',\n",
    "        'XL': 'Large',\n",
    "        'XXL': 'Large',\n",
    "        'XXXL': 'Large'\n",
    "    }\n",
    "\n",
    "    _data_['Employee_Grouped'] = _data_['Employee'].map(employee_mapping)\n",
    "\n",
    "\n",
    "\n",
    "    # Extract rows with missing 'Company_Score' values for prediction\n",
    "    X_test = _data_[pd.isnull(_data_['Company_Score'])][['Company_Score']]\n",
    "    y_train = _data_.dropna(subset=['Company_Score'])['Company_Score']\n",
    "\n",
    "    # Use only 'Company_Score' for training\n",
    "    X_train = y_train.to_frame()\n",
    "\n",
    "    # a pipeline with imputation and the model\n",
    "    model = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        RandomForestRegressor()\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # predictions on the test set\n",
    "    predicted_scores = model.predict(X_test)\n",
    "\n",
    "    # Impute missing values with predicted scores\n",
    "    _data_.loc[pd.isnull(_data_['Company_Score']), 'Company_Score'] = predicted_scores\n",
    "\n",
    "    # Mean Squared Error on the training set\n",
    "    train_predictions = model.predict(X_train)\n",
    "    train_mse = mean_squared_error(y_train, train_predictions)\n",
    "\n",
    "    print(f'Training MSE: {train_mse}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract rows with missing 'Review' values for prediction\n",
    "    X_test_review = _data_[pd.isnull(_data_['Reviews'])][['Reviews']]\n",
    "    y_train_review = _data_.dropna(subset=['Reviews'])['Reviews']\n",
    "\n",
    "    # Use only 'Review' for training\n",
    "    X_train_review = y_train_review.to_frame()\n",
    "\n",
    "    # Create a pipeline with imputation and the model\n",
    "    model_review = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        RandomForestRegressor()\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model_review.fit(X_train_review, y_train_review)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predicted_reviews = model_review.predict(X_test_review)\n",
    "\n",
    "    # Impute missing values with predicted scores\n",
    "    _data_.loc[pd.isnull(_data_['Reviews']), 'Reviews'] = predicted_reviews\n",
    "\n",
    "    # Calculate Mean Squared Error on the training set\n",
    "    train_predictions_review = model_review.predict(X_train_review)\n",
    "    train_mse_review = mean_squared_error(y_train_review, train_predictions_review)\n",
    "\n",
    "    print(f'Training MSE for Reviews: {train_mse_review}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract rows with missing 'Director_Score' values for prediction\n",
    "    X_test_director_score = _data_[pd.isnull(_data_['Director_Score'])][['Company_Score', 'Reviews']]\n",
    "    y_train_director_score = _data_.dropna(subset=['Director_Score'])['Director_Score']\n",
    "\n",
    "    # Use only 'Director_Score', 'Company_Score', and 'Reviews' for training\n",
    "    X_train_director_score = _data_.dropna(subset=['Director_Score'])[['Company_Score', 'Reviews']]\n",
    "\n",
    "    # Create a pipeline with imputation and the model\n",
    "    model_director_score = make_pipeline(\n",
    "        SimpleImputer(strategy='mean'),\n",
    "        RandomForestRegressor()\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model_director_score.fit(X_train_director_score, y_train_director_score)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predicted_director_scores = model_director_score.predict(X_test_director_score)\n",
    "\n",
    "    # Impute missing values with predicted scores\n",
    "    _data_.loc[pd.isnull(_data_['Director_Score']), 'Director_Score'] = predicted_director_scores\n",
    "\n",
    "    # Calculate Mean Squared Error on the training set\n",
    "    train_predictions_director_score = model_director_score.predict(X_train_director_score)\n",
    "    train_mse_director_score = mean_squared_error(y_train_director_score, train_predictions_director_score)\n",
    "\n",
    "    print(f'Training MSE for Director_Score: {train_mse_director_score}')\n",
    "\n",
    "\n",
    "\n",
    "    _data_.drop(['Revenue','Employee'],axis = 1,inplace = True)\n",
    "\n",
    "    _data_categorical = _data_[_data_.select_dtypes(include=['object']).columns]\n",
    "    _data_numerical = _data_[_data_.select_dtypes(exclude=['object']).columns]\n",
    "\n",
    "    _data_categorical['Skills'] = _data_categorical['Skills'].apply(eval)\n",
    "\n",
    "    # Get the unique skills\n",
    "    all_skills = set(skill for skills_list in _data_categorical['Skills'] for skill in skills_list)\n",
    "\n",
    "    # Create binary columns for each skill and initialize with zeros\n",
    "    for skill in all_skills:\n",
    "        _data_categorical[skill] = 0\n",
    "\n",
    "    # Update the binary columns based on the presence of each skill in the 'Skills' column\n",
    "    for index, row in _data_categorical.iterrows():\n",
    "        skills_list = row['Skills']\n",
    "        for skill in skills_list:\n",
    "            _data_categorical.at[index, skill] = 1\n",
    "\n",
    "    # Drop the original 'Skills' column\n",
    "    _data_categorical.drop('Skills', axis=1,inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    _data_categorical['Jobs_Group'] = enc.fit_transform(_data_categorical['Jobs_Group'])\n",
    "    _data_categorical['Sector_Group'] = enc.fit_transform(_data_categorical['Sector_Group'])\n",
    "\n",
    "\n",
    "\n",
    "    profile_mapping = {'Lead': 1, 'Senior': 2, 'Junior': 3,'Not Specified': 0}\n",
    "    remote_mapping = {'Remote': 1, 'Hybrid': 2, 'Not Specified': 0}\n",
    "    frequency_mapping = {'hour': 0, 'day': 1, 'week': 2, 'month': 3 , 'year': 4}\n",
    "    employee_mapping = {'Small' : 0,'Medium': 1,'Large': 2}\n",
    "    revenue_mapping = {'Low' : 0,'Medium': 1,'High': 2}\n",
    "\n",
    "    _data_categorical['Profile'] = _data_categorical['Profile'].map(profile_mapping)\n",
    "    _data_categorical['Remote'] = _data_categorical['Remote'].map(remote_mapping)\n",
    "    _data_categorical['Frecuency_Salary'] = _data_categorical['Frecuency_Salary'].map(frequency_mapping)\n",
    "    _data_categorical['Employee_Grouped'] = _data_categorical['Employee_Grouped'].map(employee_mapping)\n",
    "    _data_categorical['Revenue_Grouped'] = _data_categorical['Revenue_Grouped'].map(revenue_mapping)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def scale_dataframe(df):\n",
    "        # Select only numerical columns\n",
    "        numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "        # Create a MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # Scale the numerical columns\n",
    "        df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "        return df\n",
    "    \n",
    "    scaled_data_categorical = scale_dataframe(_data_categorical)\n",
    "    scaled_data_numerical = scale_dataframe(_data_numerical)\n",
    "    \n",
    "    \n",
    "    preprocessed_data = pd.concat([scaled_data_categorical, scaled_data_numerical], axis=1)\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 2.8587764436738566e-10\n",
      "Training MSE for Reviews: 25.770179527076902\n",
      "Training MSE for Director_Score: 0.0005357843439898838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical['Skills'] = _data_categorical['Skills'].apply(eval)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 8.036479373238199e-08\n",
      "Training MSE for Reviews: 103.70498601419932\n",
      "Training MSE for Director_Score: 0.0006065313264263027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical['Skills'] = _data_categorical['Skills'].apply(eval)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:195: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  _data_categorical[skill] = 0\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2280\\1985842077.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_data = preprocessing(train_data)\n",
    "preprocessed_test_data = preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33248, 109)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22166, 108)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs_Group</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Remote</th>\n",
       "      <th>Frecuency_Salary</th>\n",
       "      <th>Sector_Group</th>\n",
       "      <th>Revenue_Grouped</th>\n",
       "      <th>Employee_Grouped</th>\n",
       "      <th>Kaggle</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Streamlit</th>\n",
       "      <th>...</th>\n",
       "      <th>Qlik</th>\n",
       "      <th>Snaplogic</th>\n",
       "      <th>Fabric</th>\n",
       "      <th>Google Sheets</th>\n",
       "      <th>Essbase</th>\n",
       "      <th>SciKit</th>\n",
       "      <th>Mean_Salary</th>\n",
       "      <th>Company_Score</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Director_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183744</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.895594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305314</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130775</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.586596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.177883</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162366</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.840426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33243</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066006</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.654469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33244</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122510</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.744681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33245</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.192268</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.018494</td>\n",
       "      <td>0.902979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33246</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183744</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.590713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33247</th>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253213</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.804574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33248 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Jobs_Group   Profile  Remote  Frecuency_Salary  Sector_Group  \\\n",
       "0               9  0.000000     0.5               0.0             9   \n",
       "1               4  0.000000     0.0               1.0             9   \n",
       "2               9  0.666667     0.0               1.0            11   \n",
       "3               2  0.666667     1.0               1.0             2   \n",
       "4              11  0.000000     0.5               0.0             3   \n",
       "...           ...       ...     ...               ...           ...   \n",
       "33243           8  0.000000     0.0               1.0             3   \n",
       "33244           8  0.000000     0.0               1.0             3   \n",
       "33245           9  0.000000     0.0               1.0             3   \n",
       "33246           1  0.000000     0.0               0.0             6   \n",
       "33247           5  0.333333     0.0               1.0             8   \n",
       "\n",
       "       Revenue_Grouped  Employee_Grouped  Kaggle  Looker  Streamlit  ...  \\\n",
       "0                  1.0               0.0     0.0     0.0        0.0  ...   \n",
       "1                  1.0               0.5     0.0     0.0        0.0  ...   \n",
       "2                  0.0               0.0     0.0     0.0        0.0  ...   \n",
       "3                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "4                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "...                ...               ...     ...     ...        ...  ...   \n",
       "33243              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "33244              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "33245              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "33246              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "33247              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "\n",
       "       Qlik  Snaplogic  Fabric  Google Sheets  Essbase  SciKit  Mean_Salary  \\\n",
       "0       0.0        0.0     0.0            0.0      0.0     0.0     0.183744   \n",
       "1       1.0        0.0     0.0            0.0      0.0     0.0     0.305314   \n",
       "2       0.0        0.0     0.0            0.0      0.0     0.0     0.130775   \n",
       "3       0.0        0.0     0.0            0.0      0.0     0.0     0.177883   \n",
       "4       0.0        0.0     0.0            0.0      0.0     0.0     0.162366   \n",
       "...     ...        ...     ...            ...      ...     ...          ...   \n",
       "33243   0.0        0.0     0.0            0.0      0.0     0.0     0.066006   \n",
       "33244   0.0        0.0     0.0            0.0      0.0     0.0     0.122510   \n",
       "33245   0.0        0.0     0.0            0.0      0.0     0.0     0.192268   \n",
       "33246   0.0        0.0     0.0            0.0      0.0     0.0     0.183744   \n",
       "33247   0.0        0.0     0.0            0.0      0.0     0.0     0.253213   \n",
       "\n",
       "       Company_Score   Reviews  Director_Score  \n",
       "0              0.800  0.000077        0.895594  \n",
       "1              0.625  0.000230        0.680851  \n",
       "2              0.600  0.000351        0.586596  \n",
       "3              0.700  0.000117        0.680851  \n",
       "4              0.800  0.000746        0.840426  \n",
       "...              ...       ...             ...  \n",
       "33243          0.650  0.000149        0.654469  \n",
       "33244          0.575  0.000117        0.744681  \n",
       "33245          0.825  0.018494        0.902979  \n",
       "33246          0.675  0.010979        0.590713  \n",
       "33247          0.625  0.000012        0.804574  \n",
       "\n",
       "[33248 rows x 109 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jobs_Group</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Remote</th>\n",
       "      <th>Frecuency_Salary</th>\n",
       "      <th>Sector_Group</th>\n",
       "      <th>Revenue_Grouped</th>\n",
       "      <th>Employee_Grouped</th>\n",
       "      <th>Kaggle</th>\n",
       "      <th>Looker</th>\n",
       "      <th>Streamlit</th>\n",
       "      <th>...</th>\n",
       "      <th>Java</th>\n",
       "      <th>Qlik</th>\n",
       "      <th>Snaplogic</th>\n",
       "      <th>Fabric</th>\n",
       "      <th>Google Sheets</th>\n",
       "      <th>Essbase</th>\n",
       "      <th>SciKit</th>\n",
       "      <th>Company_Score</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Director_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.908293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22161</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.793077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22162</th>\n",
       "      <td>9</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.597561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22163</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22164</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.817073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22165</th>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.020535</td>\n",
       "      <td>0.743902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22166 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Jobs_Group   Profile  Remote  Frecuency_Salary  Sector_Group  \\\n",
       "0               1  0.333333     0.5               1.0             7   \n",
       "1               9  0.666667     0.0               1.0             7   \n",
       "2              10  0.666667     1.0               1.0             6   \n",
       "3               7  0.000000     0.5               1.0             7   \n",
       "4               9  0.000000     0.0               1.0             6   \n",
       "...           ...       ...     ...               ...           ...   \n",
       "22161           4  0.000000     0.5               1.0             7   \n",
       "22162           9  0.666667     0.0               1.0             7   \n",
       "22163           0  0.000000     0.0               0.0             4   \n",
       "22164           7  0.000000     1.0               1.0             0   \n",
       "22165          10  0.666667     0.5               1.0             7   \n",
       "\n",
       "       Revenue_Grouped  Employee_Grouped  Kaggle  Looker  Streamlit  ...  \\\n",
       "0                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "1                  1.0               0.0     0.0     0.0        0.0  ...   \n",
       "2                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "3                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "4                  1.0               1.0     0.0     0.0        0.0  ...   \n",
       "...                ...               ...     ...     ...        ...  ...   \n",
       "22161              0.0               0.5     0.0     0.0        0.0  ...   \n",
       "22162              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "22163              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "22164              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "22165              1.0               1.0     0.0     0.0        0.0  ...   \n",
       "\n",
       "       Java  Qlik  Snaplogic  Fabric  Google Sheets  Essbase  SciKit  \\\n",
       "0       0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "1       0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "2       0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "3       0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "4       0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "...     ...   ...        ...     ...            ...      ...     ...   \n",
       "22161   0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "22162   0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "22163   0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "22164   0.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "22165   1.0   0.0        0.0     0.0            0.0      0.0     0.0   \n",
       "\n",
       "       Company_Score   Reviews  Director_Score  \n",
       "0              1.000  0.000004        0.908293  \n",
       "1              0.675  0.010446        0.541500  \n",
       "2              0.675  0.010446        0.541500  \n",
       "3              0.750  0.001105        0.731707  \n",
       "4              0.675  0.010446        0.541500  \n",
       "...              ...       ...             ...  \n",
       "22161          0.750  0.000117        0.793077  \n",
       "22162          0.600  0.006555        0.597561  \n",
       "22163          0.675  0.010446        0.541500  \n",
       "22164          0.750  0.002900        0.817073  \n",
       "22165          0.650  0.020535        0.743902  \n",
       "\n",
       "[22166 rows x 108 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 50/100 [02:07<02:07,  2.55s/it, Epoch=1]\n",
      "Predicting:  50%|█████     | 50/100 [00:00<00:00, 67.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0029384052393772037\n",
      "R-squared Score: 0.5008422250492962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "X = preprocessed_train_data.drop('Mean_Salary', axis=1)\n",
    "y = preprocessed_train_data['Mean_Salary']\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "\n",
    "with tqdm(total=100, desc=\"Training\") as pbar:\n",
    "    model.fit(X_train, y_train)\n",
    "    pbar.update(50)  \n",
    "    pbar.set_postfix({\"Epoch\": 1})  \n",
    "\n",
    "\n",
    "with tqdm(total=100, desc=\"Predicting\") as pbar:\n",
    "    y_pred = model.predict(X_test)\n",
    "    pbar.update(50)  \n",
    "\n",
    "    \n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Jobs_Group', 'Profile', 'Remote', 'Frecuency_Salary', 'Sector_Group',\n",
       "       'Revenue_Grouped', 'Employee_Grouped', 'Kaggle', 'Looker', 'Streamlit',\n",
       "       ...\n",
       "       'Java', 'Qlik', 'Snaplogic', 'Fabric', 'Google Sheets', 'Essbase',\n",
       "       'SciKit', 'Company_Score', 'Reviews', 'Director_Score'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_data.columns == X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_data = preprocessed_test_data[X_test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('trained_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         ID  Predicted_Target\n",
      "0       sj_99ad4f80ae7f4835          0.201234\n",
      "1      job_6ff7f1a7c400916a          0.154099\n",
      "2      job_e059d20eba88b17a          0.237814\n",
      "3      job_79f7953813b13358          0.205652\n",
      "4      job_892dfe86b96f322b          0.124135\n",
      "...                     ...               ...\n",
      "22161  job_4ab2e5201b60cc15          0.156996\n",
      "22162  job_b3a7cf40a22659ef          0.149465\n",
      "22163  job_4e69acaf903b0276          0.126640\n",
      "22164  job_816f279ea500ecd0          0.215244\n",
      "22165  job_9861d87455c4a6a6          0.258095\n",
      "\n",
      "[22166 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "y_pred_test = loaded_model.predict(preprocessed_test_data)\n",
    "\n",
    "predictions_df = pd.DataFrame({'ID':ID, 'Predicted_Target': y_pred_test})\n",
    "\n",
    "print(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Predicted_Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sj_99ad4f80ae7f4835</td>\n",
       "      <td>0.201234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  Predicted_Target\n",
       "0  sj_99ad4f80ae7f4835          0.201234"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[predictions_df['ID'] == \"sj_99ad4f80ae7f4835\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
